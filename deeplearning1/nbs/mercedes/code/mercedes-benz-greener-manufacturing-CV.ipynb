{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda: TITAN X (Pascal) (0000:03:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# preprocessing/decomposition\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis, KernelPCA\n",
    "\n",
    "# keras \n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# supportive models\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# feature selection (from supportive model)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# to make results reproducible\n",
    "seed = 42 # was 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save IDs for submission\n",
    "id_test = test['ID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape: (8418, 378)\n"
     ]
    }
   ],
   "source": [
    "# glue datasets together\n",
    "total = pd.concat([train, test], axis=0)\n",
    "print('initial shape: {}'.format(total.shape))\n",
    "\n",
    "# binary indexes for train/test set split\n",
    "is_train = ~total.y.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X0 X1  X2 X3 X4 X5 X6 X8\n",
      "0   k  v  at  a  d  u  j  o\n",
      "1   k  t  av  e  d  y  l  o\n",
      "2  az  w   n  c  d  x  j  x\n",
      "3  az  t   n  f  d  x  l  e\n",
      "4  az  v   n  f  d  h  d  n\n"
     ]
    }
   ],
   "source": [
    "# find all categorical features\n",
    "cf = total.select_dtypes(include=['object']).columns\n",
    "print total[cf].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh-encoded shape: (8418, 211)\n",
      "   X0_a  X0_aa  X0_ab  X0_ac  X0_ad  X0_ae  X0_af  X0_ag  X0_ai  X0_aj  ...   \\\n",
      "0     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "1     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "2     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "3     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "4     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "\n",
      "   X8_p  X8_q  X8_r  X8_s  X8_t  X8_u  X8_v  X8_w  X8_x  X8_y  \n",
      "0     0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     1     0  \n",
      "3     0     0     0     0     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 211 columns]\n"
     ]
    }
   ],
   "source": [
    "# make one-hot-encoding convenient way - pandas.get_dummies(df) function\n",
    "dummies = pd.get_dummies(\n",
    "    total[cf],\n",
    "    drop_first=False # you can set it = True to ommit multicollinearity (crucial for linear models)\n",
    ")\n",
    "\n",
    "print('oh-encoded shape: {}'.format(dummies.shape))\n",
    "print dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended-encoded shape: (8418, 581)\n"
     ]
    }
   ],
   "source": [
    "# get rid of old columns and append them encoded\n",
    "total = pd.concat(\n",
    "    [\n",
    "        total.drop(cf, axis=1), # drop old\n",
    "        dummies # append them one-hot-encoded\n",
    "    ],\n",
    "    axis=1 # column-wise\n",
    ")\n",
    "\n",
    "print('appended-encoded shape: {}'.format(total.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4209, 580)\n",
      "Test shape: (4209, 579)\n"
     ]
    }
   ],
   "source": [
    "# recreate train/test again, now with dropped ID column\n",
    "train, test = total[is_train].drop(['ID'], axis=1), total[~is_train].drop(['ID', 'y'], axis=1)\n",
    "\n",
    "# drop redundant objects\n",
    "del total\n",
    "\n",
    "# check shape\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 600)\n",
      "(4209, 599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krohitm/anaconda2/envs/krohitm/lib/python2.7/site-packages/sklearn/decomposition/fastica_.py:116: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "# Calculate additional features: dimensionality reduction components\n",
    "n_comp=10 # was 10\n",
    "\n",
    "# uncomment to scale data before applying decompositions\n",
    "# however, all features are binary (in [0,1] interval), i don't know if it's worth something\n",
    "train_scaled = train.drop('y', axis=1).copy()\n",
    "test_scaled = test.copy()\n",
    "'''\n",
    "ss = StandardScaler()\n",
    "ss.fit(train.drop('y', axis=1))\n",
    "\n",
    "train_scaled = ss.transform(train.drop('y', axis=1).copy())\n",
    "test_scaled = ss.transform(test.copy())\n",
    "'''\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=seed)\n",
    "pca2_results_train = pca.fit_transform(train_scaled)\n",
    "pca2_results_test = pca.transform(test_scaled)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(train_scaled)\n",
    "ica2_results_test = ica.transform(test_scaled)\n",
    "\n",
    "# Factor Analysis\n",
    "#fca = FactorAnalysis(n_components=n_comp, random_state=seed)\n",
    "#fca2_results_train = fca.fit_transform(train_scaled)\n",
    "#fca2_results_test = fca.transform(test_scaled)\n",
    "\n",
    "# Append it to dataframes\n",
    "for i in range(1, n_comp+1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    \n",
    "    #train['fca_' + str(i)] = fca2_results_train[:,i-1]\n",
    "    #test['fca_' + str(i)] = fca2_results_test[:, i-1]\n",
    "\n",
    "\n",
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         importance\n",
      "feature            \n",
      "X119       0.041776\n",
      "X118       0.044236\n",
      "pca_6      0.091539\n",
      "X315       0.128361\n",
      "X314       0.610175\n",
      "(4209, 17)\n",
      "(4209, 17)\n",
      "\n",
      "Train shape: (4209, 617)\n",
      "Test shape: (4209, 616)\n"
     ]
    }
   ],
   "source": [
    "# create augmentation by feature importances as additional features\n",
    "t = train['y']\n",
    "tr = train.drop(['y'], axis=1)\n",
    "\n",
    "# Tree-based estimators can be used to compute feature importances\n",
    "clf = GradientBoostingRegressor(\n",
    "                max_depth=4, \n",
    "                learning_rate=0.005, \n",
    "                random_state=seed, \n",
    "                subsample=0.95, \n",
    "                n_estimators=200\n",
    ")\n",
    "\n",
    "# fit regressor\n",
    "clf.fit(tr, t)\n",
    "\n",
    "# df to hold feature importances\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = tr.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "\n",
    "print features.tail()\n",
    "\n",
    "# select best features\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(tr)\n",
    "\n",
    "test_reduced = model.transform(test.copy())\n",
    "\n",
    "print train_reduced.shape\n",
    "print test_reduced.shape\n",
    "\n",
    "# dataset augmentation\n",
    "train = pd.concat([train, pd.DataFrame(train_reduced)], axis=1)\n",
    "test = pd.concat([test, pd.DataFrame(test_reduced)], axis=1)\n",
    "\n",
    "# check new shape\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define custom R2 metrics for Keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD#, Adagrad\n",
    "# base model architecture definition\n",
    "def create_model(learn_rate = 0.0001, momentum = 0.9):\n",
    "    model = Sequential()\n",
    "    #input layer\n",
    "    model.add(Dense(input_dims, input_dim=input_dims))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    # hidden layers\n",
    "    model.add(Dense(input_dims))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(input_dims//2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(input_dims//4, activation=act_func))\n",
    "    \n",
    "    # output layer (y_pred)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile this model\n",
    "    model.compile(loss='mean_squared_error', # one may use 'mean_absolute_error' as alternative\n",
    "                  optimizer=SGD(lr=learn_rate, momentum=momentum),\n",
    "                  metrics=[r2_keras] # you can add several if needed\n",
    "                 )\n",
    "    \n",
    "    # Visualize NN architecture\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 616)\n",
      "(4209, 616)\n"
     ]
    }
   ],
   "source": [
    "# X, y preparation\n",
    "X, y = train.drop('y', axis=1).values, train.y.values\n",
    "print(X.shape)\n",
    "\n",
    "# X_test preparation\n",
    "X_test = test\n",
    "print(X_test.shape)\n",
    "\n",
    "# train/validation split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.25, \n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define path to save model\n",
    "import os\n",
    "model_path = 'keras_model_CV.h5'\n",
    "\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, # was 10\n",
    "        verbose=1),\n",
    "    \n",
    "    ModelCheckpoint(\n",
    "        model_path, \n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize input dimension\n",
    "input_dims = train.shape[1]-1\n",
    "\n",
    "#activation functions for hidden layers\n",
    "act_func = 'tanh' # could be 'relu', 'sigmoid', ...\n",
    "\n",
    "# make np.seed fixed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# initialize estimator, wrap model in KerasRegressor\n",
    "#estimator = KerasRegressor(\n",
    "#    build_fn=create_model, \n",
    "#    nb_epoch=100, \n",
    "#    batch_size=10,\n",
    "#    verbose=1\n",
    "#)\n",
    "\n",
    "estimator = KerasRegressor(\n",
    "    build_fn = create_model,\n",
    "    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grid search cv to get best hyperparameters viz. batch_size, epochs\n",
    "#batch_size = [10, 20, 40, 60, 80, 100]\n",
    "#epochs = [10, 50, 80, 100]\n",
    "#learn_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "batch_size = [10, 50, 100]\n",
    "epochs = [20, 40]\n",
    "learn_rate = [0.0001, 0.001, 0.01]\n",
    "momentum = [0.0, 0.4, 0.9]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, nb_epoch=epochs, learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get results of grid search\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define path to save model\n",
    "import os\n",
    "model_path = 'keras_model_CV.h5'\n",
    "\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, # was 10\n",
    "        verbose=1),\n",
    "    \n",
    "    ModelCheckpoint(\n",
    "        model_path, \n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "def train_and_evaluate_model(model, X_tr, y_tr, X_val, y_val):\n",
    "    model.fit(\n",
    "    X_tr, \n",
    "    y_tr, \n",
    "    #nb_epoch=10, # increase it to 20-100 to get better results\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "n_folds = 5\n",
    "#data, labels, header_info = load_data()\n",
    "skf = StratifiedKFold(y, n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print \"Running Fold\", i+1, \"/\", n_folds\n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model()\n",
    "    train_and_evaluate_model(model, X[train], y[train], X[test], y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "# check performance on train set\n",
    "print('MSE train: {}'.format(mean_squared_error(y_tr, estimator.predict(X_tr))**0.5)) # mse train\n",
    "print('R^2 train: {}'.format(r2_score(y_tr, estimator.predict(X_tr)))) # R^2 train\n",
    "\n",
    "# check performance on validation set\n",
    "print('MSE val: {}'.format(mean_squared_error(y_val, estimator.predict(X_val))**0.5)) # mse val\n",
    "print('R^2 val: {}'.format(r2_score(y_val, estimator.predict(X_val)))) # R^2 val\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary check for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "# check performance on train set\n",
    "print('MSE train: {}'.format(mean_squared_error(y_tr, estimator.predict(X_tr))**0.5)) # mse train\n",
    "print('R^2 train: {}'.format(r2_score(y_tr, estimator.predict(X_tr)))) # R^2 train\n",
    "\n",
    "# check performance on validation set\n",
    "print('MSE val: {}'.format(mean_squared_error(y_val, estimator.predict(X_val))**0.5)) # mse val\n",
    "print('R^2 val: {}'.format(r2_score(y_val, estimator.predict(X_val)))) # R^2 val\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "res = estimator.predict(X_test.values).ravel()\n",
    "\n",
    "# create df and convert it to csv\n",
    "output = pd.DataFrame({'id': id_test, 'y': res})\n",
    "output.to_csv('../results/test_check.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.save_weights('mercedes_sgd_weights_x.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
