{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# preprocessing/decomposition\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis, KernelPCA\n",
    "\n",
    "# keras \n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# supportive models\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# feature selection (from supportive model)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# to make results reproducible\n",
    "seed = 42 # was 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save IDs for submission\n",
    "id_test = test['ID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape: (8418, 378)\n"
     ]
    }
   ],
   "source": [
    "# glue datasets together\n",
    "total = pd.concat([train, test], axis=0)\n",
    "print('initial shape: {}'.format(total.shape))\n",
    "\n",
    "# binary indexes for train/test set split\n",
    "is_train = ~total.y.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X0 X1  X2 X3 X4 X5 X6 X8\n",
      "0   k  v  at  a  d  u  j  o\n",
      "1   k  t  av  e  d  y  l  o\n",
      "2  az  w   n  c  d  x  j  x\n",
      "3  az  t   n  f  d  x  l  e\n",
      "4  az  v   n  f  d  h  d  n\n"
     ]
    }
   ],
   "source": [
    "# find all categorical features\n",
    "cf = total.select_dtypes(include=['object']).columns\n",
    "print total[cf].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh-encoded shape: (8418, 211)\n",
      "   X0_a  X0_aa  X0_ab  X0_ac  X0_ad  X0_ae  X0_af  X0_ag  X0_ai  X0_aj  ...   \\\n",
      "0     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "1     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "2     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "3     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "4     0      0      0      0      0      0      0      0      0      0  ...    \n",
      "\n",
      "   X8_p  X8_q  X8_r  X8_s  X8_t  X8_u  X8_v  X8_w  X8_x  X8_y  \n",
      "0     0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     1     0  \n",
      "3     0     0     0     0     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 211 columns]\n"
     ]
    }
   ],
   "source": [
    "# make one-hot-encoding convenient way - pandas.get_dummies(df) function\n",
    "dummies = pd.get_dummies(\n",
    "    total[cf],\n",
    "    drop_first=False # you can set it = True to ommit multicollinearity (crucial for linear models)\n",
    ")\n",
    "\n",
    "print('oh-encoded shape: {}'.format(dummies.shape))\n",
    "print dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended-encoded shape: (8418, 581)\n"
     ]
    }
   ],
   "source": [
    "# get rid of old columns and append them encoded\n",
    "total = pd.concat(\n",
    "    [\n",
    "        total.drop(cf, axis=1), # drop old\n",
    "        dummies # append them one-hot-encoded\n",
    "    ],\n",
    "    axis=1 # column-wise\n",
    ")\n",
    "\n",
    "print('appended-encoded shape: {}'.format(total.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4209, 580)\n",
      "Test shape: (4209, 579)\n"
     ]
    }
   ],
   "source": [
    "# recreate train/test again, now with dropped ID column\n",
    "train, test = total[is_train].drop(['ID'], axis=1), total[~is_train].drop(['ID', 'y'], axis=1)\n",
    "\n",
    "# drop redundant objects\n",
    "del total\n",
    "\n",
    "# check shape\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krohitm/anaconda2/envs/krohitm/lib/python2.7/site-packages/sklearn/decomposition/fastica_.py:116: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "# Calculate additional features: dimensionality reduction components\n",
    "n_comp=10 # was 10\n",
    "\n",
    "# uncomment to scale data before applying decompositions\n",
    "# however, all features are binary (in [0,1] interval), i don't know if it's worth something\n",
    "train_scaled = train.drop('y', axis=1).copy()\n",
    "test_scaled = test.copy()\n",
    "'''\n",
    "ss = StandardScaler()\n",
    "ss.fit(train.drop('y', axis=1))\n",
    "\n",
    "train_scaled = ss.transform(train.drop('y', axis=1).copy())\n",
    "test_scaled = ss.transform(test.copy())\n",
    "'''\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=seed)\n",
    "pca2_results_train = pca.fit_transform(train_scaled)\n",
    "pca2_results_test = pca.transform(test_scaled)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(train_scaled)\n",
    "ica2_results_test = ica.transform(test_scaled)\n",
    "\n",
    "# Factor Analysis\n",
    "fca = FactorAnalysis(n_components=n_comp, random_state=seed)\n",
    "fca2_results_train = fca.fit_transform(train_scaled)\n",
    "fca2_results_test = fca.transform(test_scaled)\n",
    "\n",
    "# Append it to dataframes\n",
    "for i in range(1, n_comp+1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    \n",
    "    #train['fca_' + str(i)] = fca2_results_train[:,i-1]\n",
    "    #test['fca_' + str(i)] = fca2_results_test[:, i-1]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         importance\n",
      "feature            \n",
      "X119       0.041776\n",
      "X118       0.044236\n",
      "pca_6      0.091539\n",
      "X315       0.128361\n",
      "X314       0.610175\n",
      "\n",
      "Train shape: (4209, 617)\n",
      "Test shape: (4209, 616)\n"
     ]
    }
   ],
   "source": [
    "# create augmentation by feature importances as additional features\n",
    "t = train['y']\n",
    "tr = train.drop(['y'], axis=1)\n",
    "\n",
    "# Tree-based estimators can be used to compute feature importances\n",
    "clf = GradientBoostingRegressor(\n",
    "                max_depth=4, \n",
    "                learning_rate=0.005, \n",
    "                random_state=seed, \n",
    "                subsample=0.95, \n",
    "                n_estimators=200\n",
    ")\n",
    "\n",
    "# fit regressor\n",
    "clf.fit(tr, t)\n",
    "\n",
    "# df to hold feature importances\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = tr.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "\n",
    "print features.tail()\n",
    "\n",
    "# select best features\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(tr)\n",
    "\n",
    "test_reduced = model.transform(test.copy())\n",
    "\n",
    "# dataset augmentation\n",
    "train = pd.concat([train, pd.DataFrame(train_reduced)], axis=1)\n",
    "test = pd.concat([test, pd.DataFrame(test_reduced)], axis=1)\n",
    "\n",
    "# check new shape\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define custom R2 metrics for Keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adagrad, Adadelta\n",
    "# base model architecture definition\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    #input layer\n",
    "    model.add(Dense(input_dims, input_dim=input_dims))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    # hidden layers\n",
    "    model.add(Dense(input_dims))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(input_dims//2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(input_dims//4, activation=act_func))\n",
    "    \n",
    "    # output layer (y_pred)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile this model\n",
    "    model.compile(loss='mean_squared_error', # one may use 'mean_absolute_error' as alternative\n",
    "                  #optimizer='adam',\n",
    "                  optimizer=Adadelta(),#SGD(lr=0.0001, momentum=0.9),\n",
    "                  #optimizer=Adagrad(),\n",
    "                  metrics=[r2_keras] # you can add several if needed\n",
    "                 )\n",
    "    \n",
    "    # Visualize NN architecture\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize input dimension\n",
    "input_dims = train.shape[1]-1\n",
    "\n",
    "#activation functions for hidden layers\n",
    "act_func = 'tanh' # could be 'relu', 'sigmoid', ...\n",
    "\n",
    "# make np.seed fixed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# initialize estimator, wrap model in KerasRegressor\n",
    "estimator = KerasRegressor(\n",
    "    build_fn=model, \n",
    "    nb_epoch=100, \n",
    "    batch_size=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 616)\n",
      "(4209, 616)\n"
     ]
    }
   ],
   "source": [
    "# X, y preparation\n",
    "X, y = train.drop('y', axis=1).values, train.y.values\n",
    "print(X.shape)\n",
    "\n",
    "# X_test preparation\n",
    "X_test = test\n",
    "print(X_test.shape)\n",
    "\n",
    "# train/validation split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.20, \n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_6 (Dense)                  (None, 616)           380072      dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 616)           2464        dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 616)           0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 616)           0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 616)           380072      dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 616)           2464        dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 616)           0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 616)           0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 308)           190036      dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 308)           1232        dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 308)           0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 308)           0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 154)           47586       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             155         dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,004,081\n",
      "Trainable params: 1,001,001\n",
      "Non-trainable params: 3,080\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 3367 samples, validate on 842 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 6394.0030 - r2_keras: -6.1273e+01 - val_loss: 1304.3339 - val_r2_keras: -1.0349e+01\n",
      "Epoch 2/100\n",
      "1s - loss: 503.2539 - r2_keras: -3.0927e+00 - val_loss: 168.5173 - val_r2_keras: -1.8353e-01\n",
      "Epoch 3/100\n",
      "1s - loss: 147.3422 - r2_keras: -6.6220e-02 - val_loss: 125.6487 - val_r2_keras: 0.0830\n",
      "Epoch 4/100\n",
      "1s - loss: 126.9811 - r2_keras: 0.1120 - val_loss: 95.1535 - val_r2_keras: 0.3354\n",
      "Epoch 5/100\n",
      "1s - loss: 106.3876 - r2_keras: 0.2417 - val_loss: 77.4507 - val_r2_keras: 0.4740\n",
      "Epoch 6/100\n",
      "1s - loss: 91.3277 - r2_keras: 0.3901 - val_loss: 70.5071 - val_r2_keras: 0.5260\n",
      "Epoch 7/100\n",
      "1s - loss: 84.9132 - r2_keras: 0.4459 - val_loss: 66.6179 - val_r2_keras: 0.5463\n",
      "Epoch 8/100\n",
      "1s - loss: 82.4771 - r2_keras: 0.4541 - val_loss: 64.6251 - val_r2_keras: 0.5578\n",
      "Epoch 9/100\n",
      "1s - loss: 78.3884 - r2_keras: 0.5041 - val_loss: 65.0006 - val_r2_keras: 0.5527\n",
      "Epoch 10/100\n",
      "1s - loss: 76.8051 - r2_keras: 0.4928 - val_loss: 63.2065 - val_r2_keras: 0.5782\n",
      "Epoch 11/100\n",
      "1s - loss: 75.2029 - r2_keras: 0.4959 - val_loss: 63.2919 - val_r2_keras: 0.5742\n",
      "Epoch 12/100\n",
      "1s - loss: 73.0743 - r2_keras: 0.4689 - val_loss: 62.9828 - val_r2_keras: 0.5764\n",
      "Epoch 13/100\n",
      "1s - loss: 73.0625 - r2_keras: 0.5353 - val_loss: 63.6272 - val_r2_keras: 0.5739\n",
      "Epoch 14/100\n",
      "1s - loss: 72.3447 - r2_keras: 0.5252 - val_loss: 64.1477 - val_r2_keras: 0.5669\n",
      "Epoch 15/100\n",
      "1s - loss: 71.8971 - r2_keras: 0.5466 - val_loss: 63.1064 - val_r2_keras: 0.5770\n",
      "Epoch 16/100\n",
      "1s - loss: 71.5791 - r2_keras: 0.5418 - val_loss: 63.4811 - val_r2_keras: 0.5746\n",
      "Epoch 17/100\n",
      "1s - loss: 69.6723 - r2_keras: 0.5608 - val_loss: 63.9210 - val_r2_keras: 0.5707\n",
      "Epoch 18/100\n",
      "1s - loss: 70.0372 - r2_keras: 0.5592 - val_loss: 63.3586 - val_r2_keras: 0.5743\n",
      "Epoch 19/100\n",
      "1s - loss: 69.1003 - r2_keras: 0.5506 - val_loss: 63.8056 - val_r2_keras: 0.5775\n",
      "Epoch 20/100\n",
      "1s - loss: 71.4056 - r2_keras: 0.4669 - val_loss: 62.9273 - val_r2_keras: 0.5838\n",
      "Epoch 21/100\n",
      "1s - loss: 71.2448 - r2_keras: 0.5313 - val_loss: 63.9540 - val_r2_keras: 0.5708\n",
      "Epoch 22/100\n",
      "1s - loss: 69.2646 - r2_keras: 0.5557 - val_loss: 63.9673 - val_r2_keras: 0.5765\n",
      "Epoch 23/100\n",
      "1s - loss: 68.0074 - r2_keras: 0.5570 - val_loss: 63.7094 - val_r2_keras: 0.5773\n",
      "Epoch 24/100\n",
      "1s - loss: 68.3254 - r2_keras: 0.5258 - val_loss: 64.0435 - val_r2_keras: 0.5786\n",
      "Epoch 25/100\n",
      "1s - loss: 67.1165 - r2_keras: 0.5748 - val_loss: 62.7844 - val_r2_keras: 0.5779\n",
      "Epoch 26/100\n",
      "1s - loss: 65.9514 - r2_keras: 0.5655 - val_loss: 63.2709 - val_r2_keras: 0.5742\n",
      "Epoch 27/100\n",
      "1s - loss: 66.1189 - r2_keras: 0.5762 - val_loss: 63.3663 - val_r2_keras: 0.5744\n",
      "Epoch 28/100\n",
      "1s - loss: 66.3437 - r2_keras: 0.6044 - val_loss: 63.3348 - val_r2_keras: 0.5769\n",
      "Epoch 29/100\n",
      "1s - loss: 65.7923 - r2_keras: 0.5729 - val_loss: 63.6869 - val_r2_keras: 0.5764\n",
      "Epoch 30/100\n",
      "1s - loss: 65.0994 - r2_keras: 0.5690 - val_loss: 64.6346 - val_r2_keras: 0.5713\n",
      "Epoch 31/100\n",
      "1s - loss: 66.0020 - r2_keras: 0.5815 - val_loss: 65.0727 - val_r2_keras: 0.5648\n",
      "Epoch 32/100\n",
      "1s - loss: 64.7405 - r2_keras: 0.5915 - val_loss: 65.1075 - val_r2_keras: 0.5627\n",
      "Epoch 33/100\n",
      "1s - loss: 64.8265 - r2_keras: 0.5809 - val_loss: 65.7094 - val_r2_keras: 0.5666\n",
      "Epoch 34/100\n",
      "1s - loss: 65.1444 - r2_keras: 0.5663 - val_loss: 64.3614 - val_r2_keras: 0.5742\n",
      "Epoch 35/100\n",
      "1s - loss: 64.4480 - r2_keras: 0.5845 - val_loss: 66.5804 - val_r2_keras: 0.5526\n",
      "Epoch 36/100\n",
      "1s - loss: 65.1562 - r2_keras: 0.5623 - val_loss: 65.7460 - val_r2_keras: 0.5591\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc71e8bb50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define path to save model\n",
    "import os\n",
    "model_path = 'keras_model_adadelta.h5'\n",
    "\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, # was 10\n",
    "        verbose=1),\n",
    "    \n",
    "    ModelCheckpoint(\n",
    "        model_path, \n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        verbose=0)\n",
    "]\n",
    "\n",
    "# fit estimator\n",
    "estimator.fit(\n",
    "    X_tr, \n",
    "    y_tr, \n",
    "    #nb_epoch=10, # increase it to 20-100 to get better results\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 7.80798918632\n",
      "R^2 train: 0.623668164219\n",
      "MSE val: 7.92366340175\n",
      "R^2 val: 0.596630478318\n"
     ]
    }
   ],
   "source": [
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "# check performance on train set\n",
    "print('MSE train: {}'.format(mean_squared_error(y_tr, estimator.predict(X_tr))**0.5)) # mse train\n",
    "print('R^2 train: {}'.format(r2_score(y_tr, estimator.predict(X_tr)))) # R^2 train\n",
    "\n",
    "# check performance on validation set\n",
    "print('MSE val: {}'.format(mean_squared_error(y_val, estimator.predict(X_val))**0.5)) # mse val\n",
    "print('R^2 val: {}'.format(r2_score(y_val, estimator.predict(X_val)))) # R^2 val\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary check for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 7.80798918632\n",
      "R^2 train: 0.623668164219\n",
      "MSE val: 7.92366340175\n",
      "R^2 val: 0.596630478318\n"
     ]
    }
   ],
   "source": [
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "# check performance on train set\n",
    "print('MSE train: {}'.format(mean_squared_error(y_tr, estimator.predict(X_tr))**0.5)) # mse train\n",
    "print('R^2 train: {}'.format(r2_score(y_tr, estimator.predict(X_tr)))) # R^2 train\n",
    "\n",
    "# check performance on validation set\n",
    "print('MSE val: {}'.format(mean_squared_error(y_val, estimator.predict(X_val))**0.5)) # mse val\n",
    "print('R^2 val: {}'.format(r2_score(y_val, estimator.predict(X_val)))) # R^2 val\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "res = estimator.predict(X_test.values).ravel()\n",
    "\n",
    "# create df and convert it to csv\n",
    "output = pd.DataFrame({'id': id_test, 'y': res})\n",
    "output.to_csv('../results/adadelta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 91.89006805,  91.54103088,  93.6187439 , ...,  93.46170044,\n",
       "        92.79412079,  95.83500671], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.predict(X_tr).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 378)\n",
      "Shape test: (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "n_comp = 10\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    \n",
    "y_train = train[\"y\"]\n",
    "y_mean = np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.6399+0.154977\ttest-rmse:12.6383+0.309394\n",
      "[50]\ttrain-rmse:11.0903+0.173162\ttest-rmse:11.1515+0.321941\n",
      "[100]\ttrain-rmse:10.0181+0.193437\ttest-rmse:10.1468+0.339326\n",
      "[150]\ttrain-rmse:9.28968+0.212088\ttest-rmse:9.48773+0.359611\n",
      "[200]\ttrain-rmse:8.80275+0.226347\ttest-rmse:9.06668+0.37999\n",
      "[250]\ttrain-rmse:8.47847+0.236952\ttest-rmse:8.80237+0.39653\n",
      "[300]\ttrain-rmse:8.26052+0.243699\ttest-rmse:8.63721+0.408339\n",
      "[350]\ttrain-rmse:8.09265+0.240265\ttest-rmse:8.53675+0.417034\n",
      "[400]\ttrain-rmse:7.94872+0.223081\ttest-rmse:8.47722+0.423239\n",
      "[450]\ttrain-rmse:7.83237+0.208945\ttest-rmse:8.44253+0.428947\n",
      "[500]\ttrain-rmse:7.72702+0.191756\ttest-rmse:8.4231+0.431914\n",
      "[550]\ttrain-rmse:7.63446+0.174185\ttest-rmse:8.41282+0.433279\n",
      "[600]\ttrain-rmse:7.55366+0.156574\ttest-rmse:8.40588+0.433408\n",
      "[649]\ttrain-rmse:7.48377+0.14259\ttest-rmse:8.40391+0.43269\n",
      "650\n"
     ]
    }
   ],
   "source": [
    " ()# mmm, xgboost, loved by everyone ^-^\n",
    "import xgboost as xgb\n",
    "\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = {\n",
    "    'n_trees': 500, \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.95,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=650, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=50, \n",
    "                   show_stdv=True\n",
    "                  )\n",
    "\n",
    "num_boost_rounds = len(cv_result)\n",
    "print(num_boost_rounds)\n",
    "\n",
    "# train model\n",
    "xgb_model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63188978762\n"
     ]
    }
   ],
   "source": [
    "# check f2-score (to get higher score - increase num_boost_round in previous cell)\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# now fixed, correct calculation\n",
    "print(r2_score(dtrain.get_label(), xgb_model.predict(dtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model.save_model('xgb_{}.model'.format(num_boost_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make predictions and save results\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\n",
    "output.to_csv('xgboost-boost_rounds{}-pca-ica.csv'.format(num_boost_rounds), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trying to ensemble the results of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_train_preds = xgb_model.predict(dtrain)\n",
    "keras_train_preds = estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_test_preds = xgb_model.predict(dtest)\n",
    "keras_test_preds = estimator.predict(X_test.values).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cum_train_preds = np.column_stack((keras_train_preds, xgb_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cum_test_preds = np.column_stack((keras_test_preds, xgb_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.6399+0.154977\ttest-rmse:12.6383+0.309394\n",
      "[50]\ttrain-rmse:11.0903+0.173162\ttest-rmse:11.1515+0.321941\n",
      "[100]\ttrain-rmse:10.0181+0.193437\ttest-rmse:10.1468+0.339326\n",
      "[150]\ttrain-rmse:9.28968+0.212088\ttest-rmse:9.48773+0.359611\n",
      "[200]\ttrain-rmse:8.80275+0.226347\ttest-rmse:9.06668+0.37999\n",
      "[250]\ttrain-rmse:8.47847+0.236952\ttest-rmse:8.80237+0.39653\n",
      "[300]\ttrain-rmse:8.26052+0.243699\ttest-rmse:8.63721+0.408339\n",
      "[350]\ttrain-rmse:8.09265+0.240265\ttest-rmse:8.53675+0.417034\n",
      "[400]\ttrain-rmse:7.94872+0.223081\ttest-rmse:8.47722+0.423239\n",
      "[450]\ttrain-rmse:7.83237+0.208945\ttest-rmse:8.44253+0.428947\n",
      "[500]\ttrain-rmse:7.72702+0.191756\ttest-rmse:8.4231+0.431914\n",
      "[550]\ttrain-rmse:7.63446+0.174185\ttest-rmse:8.41282+0.433279\n",
      "[600]\ttrain-rmse:7.55366+0.156574\ttest-rmse:8.40588+0.433408\n",
      "[650]\ttrain-rmse:7.48207+0.14231\ttest-rmse:8.40394+0.4326\n",
      "[699]\ttrain-rmse:7.41426+0.127975\ttest-rmse:8.40462+0.430224\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params_new = {\n",
    "    'n_trees': 500, \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.95,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain_new = xgb.DMatrix(cum_train_preds, y_train)\n",
    "dtest_new = xgb.DMatrix(cum_test_preds)\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result_new = xgb.cv(xgb_params_new, \n",
    "                       dtrain, \n",
    "                       num_boost_round=500, # increase to have better results (~700)\n",
    "                       early_stopping_rounds=50,\n",
    "                       verbose_eval=50, \n",
    "                       show_stdv=True\n",
    "                      )\n",
    "\n",
    "num_boost_rounds_new = len(cv_result_new)\n",
    "print(num_boost_rounds_new)\n",
    "\n",
    "# train model\n",
    "xgb_model_new = xgb.train(dict(xgb_params_new, silent=0), dtrain_new, num_boost_round=num_boost_rounds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714911383705\n"
     ]
    }
   ],
   "source": [
    "# now fixed, correct calculation\n",
    "print(r2_score(dtrain_new.get_label(), xgb_model_new.predict(dtrain_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model_new.save_model('xgb_{}_ensemble.model'.format(num_boost_rounds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions and save results\n",
    "y_pred_new = xgb_model_new.predict(dtest_new)\n",
    "output_new = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred_new})\n",
    "output_new.to_csv('xgboost-boost_rounds{}-pca-ica_ensemble.csv'.format(num_boost_rounds_new), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
